p8105\_hw3\_bsr2136
================
Barik Rajpal
10/10/2019

## Problem 0

``` r
dir.create("files_for_hw3")
```

## Problem 1

### Describing the dataset

``` r
library(p8105.datasets)
data("instacart")

## The number of rows in the dataset is
nrow(instacart)
```

    ## [1] 1384617

``` r
## The number of columns in the dataset is
ncol(instacart)
```

    ## [1] 15

``` r
## The dataset has identifying information such as order, product, and user.
## For example, The number of unique orders in the dataset is
length(unique(pull(instacart,order_id)))
```

    ## [1] 131209

``` r
## Which is equal to the number of unique users
length(unique(pull(instacart,user_id)))
```

    ## [1] 131209

``` r
## And the number of unique products is
length(unique(pull(instacart,product_id)))
```

    ## [1] 39123

``` r
## The dataset also gives information such as aisle and department of the
## product ordered. For example, The departments are
unique(pull(instacart,department))
```

    ##  [1] "dairy eggs"      "produce"         "canned goods"   
    ##  [4] "beverages"       "deli"            "snacks"         
    ##  [7] "pantry"          "frozen"          "meat seafood"   
    ## [10] "household"       "bakery"          "personal care"  
    ## [13] "dry goods pasta" "babies"          "missing"        
    ## [16] "other"           "breakfast"       "international"  
    ## [19] "alcohol"         "bulk"            "pets"

``` r
## The dataset also gives the hour of day that the order was made.
## For example, the most common hour at which an order was made is
instacart %>% group_by(order_hour_of_day) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  .[1,1]
```

    ## # A tibble: 1 x 1
    ##   order_hour_of_day
    ##               <int>
    ## 1                14

### Answering prompts about the dataset

#### 1\. How many aisles are there, and which aisles are the most items ordered from?

``` r
## The number of aisles are
length(unique(pull(instacart,aisle)))
```

    ## [1] 134

``` r
## And the aisles that the most items are ordered from are
instacart %>% group_by(aisle) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head()
```

    ## # A tibble: 6 x 2
    ##   aisle                              n
    ##   <chr>                          <int>
    ## 1 fresh vegetables              150609
    ## 2 fresh fruits                  150473
    ## 3 packaged vegetables fruits     78493
    ## 4 yogurt                         55240
    ## 5 packaged cheese                41699
    ## 6 water seltzer sparkling water  36617

There are 134 aisles total, and the 6 aisles that foods are most
commonly ordered from are fresh vegetables, fresh fruits, packaged
vegetables fruits, yogurt, packaged cheese, and water seltzer sparkling
water. Among these 6, the top 2, fresh vegetables and fresh fruit,
receive far more orders than the other 4 (almost 2 times the 3rd ranked
aisle, packaged vegetables
fruits)

#### 2\. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

``` r
aisle_tbl <- instacart %>% 
  group_by(aisle) %>%
  summarise(n = n()) %>%
  filter(n>=10000) %>%
  arrange(n)

aisle_tbl %>%
  mutate(
    aisle = factor(pull(aisle_tbl,aisle),
                levels = pull(aisle_tbl,aisle)[order(pull(aisle_tbl,n))])) %>% 
  ggplot(aes(x=aisle,y=n)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + xlab("Aisle") + ylab("Number of Items Ordered")
```

![](p8105_hw3_bsr2136_files/figure-gfm/Problem%201.2-1.png)<!-- -->

As also seen in the previous question, fresh vegetables and fresh fruits
are ordered from far more than any other aisle. While there is a steep
drop in number of items ordered after the highest 2 aisles, then a few
more moderate drops over the next few aisles, the rate of decay slows
greaty and from the eggs aisle to the butter aisle, (which looks to be
~25 aisles) there is a very small difference in the number of items
ordered per
aisle.

#### 3\. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

``` r
baking <- instacart %>%
  filter(aisle == "baking ingredients") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

dog_food <- instacart %>%
  filter(aisle == "dog food care") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

pack_veg <- instacart %>%
  filter(aisle == "packaged vegetables fruits") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

rbind(baking,dog_food,pack_veg) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 | number\_of\_times\_ordered |
| :------------------------- | :-------------------------------------------- | -------------------------: |
| baking ingredients         | Light Brown Sugar                             |                        499 |
| baking ingredients         | Pure Baking Soda                              |                        387 |
| baking ingredients         | Cane Sugar                                    |                        336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |                         30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |                         28 |
| dog food care              | Small Dog Biscuits                            |                         26 |
| packaged vegetables fruits | Organic Baby Spinach                          |                       9784 |
| packaged vegetables fruits | Organic Raspberries                           |                       5546 |
| packaged vegetables fruits | Organic Blueberries                           |                       4966 |

In addition to giving information about what are the 3 most popular
items in each of the three aisles, this table gives very clear
information on how popular the aisles are as well. The packaged
vegetables fruits aisle is ordered from far more than the baking
ingredients aisle, which is ordered from far more than the dog food care
aisle.

#### 4\. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

``` r
day_of_week <- instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name,order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow,
              values_from = mean_hour)

names(day_of_week)[2:8] <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")

knitr::kable(day_of_week)
```

| product\_name    |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
| :--------------- | -------: | -------: | -------: | --------: | -------: | -------: | -------: |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 |  15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 |  14.25000 | 11.55172 | 12.78431 | 11.93750 |

The mean time at which Coffee Ice Cream is ordered is between 1 and 2 on
the weekend, between 3 and 4 from Tuesday through Thursday. Throughout
the week, the mean time is between 12 and 4.

The mean time at which Pink Lady Apples are ordered is between 11 and 12
Monday, Tuesday, Thursday, and Saturday, and between 11 and 230 through
the week.

## Problem 2

### Data Loading and Cleaning

``` r
library(p8105.datasets)
data("brfss_smart2010")

unique(pull(brfss_smart2010,Locationabbr))
```

    ##  [1] "AL" "AZ" "AR" "CA" "CO" "CT" "DE" "DC" "FL" "GA" "HI" "ID" "IL" "IN"
    ## [15] "IA" "KS" "KY" "LA" "ME" "MD" "MA" "MI" "MN" "MO" "MS" "MT" "NE" "NV"
    ## [29] "NH" "NJ" "NM" "NY" "NC" "OH" "ND" "OK" "OR" "PA" "RI" "SC" "SD" "TN"
    ## [43] "TX" "UT" "VT" "WA" "WI" "WV" "WY" "AK" "VA"

``` r
## This is just state, so I'm going to call it that

## I checked Locationdesc the same way, but I'm not showing it here to
## save space on the github_document.
## It is just county, so I'm going to call it that


names(brfss_smart2010)[2:3] = c("state","county")

brfss <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Poor","Fair","Good","Very good","Excellent")) %>%
  mutate(response = factor(response, levels = c("Poor","Fair","Good","Very good","Excellent")))

unique(pull(brfss,data_value_unit))
```

    ## [1] "%"

``` r
## So I'm going to get rid of that column and rename data_value to indicate that it is a %
unique(pull(brfss,location_id))
```

    ## [1] NA

``` r
unique(pull(brfss,data_value_footnote_symbol))
```

    ## [1] NA  "*"

``` r
## And get rid of those as well

names(brfss)[9] = "percent_responded"

brfss <- brfss %>%
  select(-topic,-data_value_unit,-location_id,-data_value_footnote_symbol)
```

### Answering questions about the dataset

#### 1\. In 2002, which states were observed at 7 or more locations? What about in 2010?

``` r
brfss %>%
  filter(year==2002) %>%
  distinct(state,county) %>%
  group_by(state) %>%
  summarise(number_of_locations = n()) %>%
  filter(number_of_locations >=7)
```

    ## # A tibble: 6 x 2
    ##   state number_of_locations
    ##   <chr>               <int>
    ## 1 CT                      7
    ## 2 FL                      7
    ## 3 MA                      8
    ## 4 NC                      7
    ## 5 NJ                      8
    ## 6 PA                     10

``` r
brfss %>%
  filter(year==2010) %>%
  distinct(state,county) %>%
  group_by(state) %>%
  summarise(number_of_locations = n()) %>%
  filter(number_of_locations >=7)
```

    ## # A tibble: 14 x 2
    ##    state number_of_locations
    ##    <chr>               <int>
    ##  1 CA                     12
    ##  2 CO                      7
    ##  3 FL                     41
    ##  4 MA                      9
    ##  5 MD                     12
    ##  6 NC                     12
    ##  7 NE                     10
    ##  8 NJ                     19
    ##  9 NY                      9
    ## 10 OH                      8
    ## 11 PA                      7
    ## 12 SC                      7
    ## 13 TX                     16
    ## 14 WA                     10

In 2002, there were 6 states that were observed at 7 or more locations.
PA was observed at 10 locations, NJ and MA were observed at 8 locations,
and CT, FL, and NC were observed at 7 locations. In 2010, there were 14
states that were observed at 7 or more locations. FL, which was only
observed in 7 locations in 2002, was observed at 41 locations in 2010,
more than any other state. The others that were observed in 7 or more in
2002 were all also observed for 7 or more in 2010 except for
CT.

#### 2\. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data\_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom\_line geometry and group aesthetic will help).

``` r
brfss %>%
  filter(response == "Excellent") %>%
  filter(!is.na(percent_responded)) %>%
  group_by(year,state) %>%
  summarise(percent_excellent = mean(percent_responded)) %>%
  ggplot(aes(x=year,y=percent_excellent,color=state)) + 
  geom_line(aes(group = state))
```

![](p8105_hw3_bsr2136_files/figure-gfm/Problem%202.2-1.png)<!-- -->

The state (location) that consistently has close to the highest percent
responding with excellent is DC (which is not techincally a state), and
the state that consistently has close to the lowest percent responding
with excellent is WV. While there is a lot of movement (or noise) over
the years, states do not seem to dramatically switch ranks in terms of
percent responding excellent over
time.

#### 3\. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data\_value for responses (“Poor” to “Excellent”) among locations in NY State.

``` r
brfss_2006 <- brfss %>%
  filter(year==2006 & state == "NY") %>%
  ggplot(aes(x=response,y=percent_responded)) + 
  geom_boxplot() + ggtitle("2006") + 
  scale_y_continuous(limits = c(0,45))

brfss_2010 <- brfss %>%
  filter(year==2010 & state == "NY") %>%
  ggplot(aes(x=response,y=percent_responded)) + 
  geom_boxplot() + ggtitle("2010") + 
  scale_y_continuous(limits = c(0,45))

gridExtra::grid.arrange(brfss_2006,brfss_2010, nrow=1)
```

![](p8105_hw3_bsr2136_files/figure-gfm/problem%202.3-1.png)<!-- -->

From 2006 to 2010, the amount of “Excellent” and “Very good” responses
incrased on average, while the amount of “Fair” responses decreased on
average.

## Problem 3

### Loading, tidying, and wrangling the data

``` r
accel <- read_csv("./files_for_hw3/accel_data.csv") %>%
  mutate(weekend = ifelse(day %in% c("Saturday","Sunday"),1,0),
         day = factor(day, levels = c("Sunday","Monday","Tuesday","Wednesday",
                                      "Thursday","Friday","Saturday"))) %>%
  select(weekend,everything()) %>%
  janitor::clean_names()
```

The resulting dataset has 35 observations of 1444 varibles, 1440 of
those variables are the accelerator minute data. The other 4 are
weekend, which is a boolean that indicates whether the day is a weekend,
week, which is a numeric that indicates what week it is (1-5), day\_id,
which is a numeric that indicates what day it is (1-35), and day, which
is a factor that indicates what day of the week it is (Sunday, Monday,
etc.).

### Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

``` r
accel_agg <- accel %>%
  mutate(total_activity = rowSums(accel[5:1444])) %>%
  select(day_id, total_activity)

accel_agg %>%
  knitr::kable()
```

| day\_id | total\_activity |
| ------: | --------------: |
|       1 |       480542.62 |
|       2 |        78828.07 |
|       3 |       376254.00 |
|       4 |       631105.00 |
|       5 |       355923.64 |
|       6 |       307094.24 |
|       7 |       340115.01 |
|       8 |       568839.00 |
|       9 |       295431.00 |
|      10 |       607175.00 |
|      11 |       422018.00 |
|      12 |       474048.00 |
|      13 |       423245.00 |
|      14 |       440962.00 |
|      15 |       467420.00 |
|      16 |       685910.00 |
|      17 |       382928.00 |
|      18 |       467052.00 |
|      19 |       371230.00 |
|      20 |       381507.00 |
|      21 |       468869.00 |
|      22 |       154049.00 |
|      23 |       409450.00 |
|      24 |         1440.00 |
|      25 |       260617.00 |
|      26 |       340291.00 |
|      27 |       319568.00 |
|      28 |       434460.00 |
|      29 |       620860.00 |
|      30 |       389080.00 |
|      31 |         1440.00 |
|      32 |       138421.00 |
|      33 |       549658.00 |
|      34 |       367824.00 |
|      35 |       445366.00 |

The first thing that stands out to me is that on day 24 and day 31,
there total\_activity values is 1440, which is the total number of
minutes, which makes me suspect that the device was malfunctioning that
day. Regarding trends, a high value is often followed by one or two low
values, which maybe suggests that after 1 day of exercise, there was 1-2
days of
rest.

### Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

``` r
accel %>%
  pivot_longer(activity_1:activity_1440,
               names_to = "time_of_day",
               values_to = "activity") %>%
  mutate(time_of_day = as.numeric(
    str_replace(time_of_day,"activity_",""))) %>%
  ggplot(aes(x=time_of_day,y=activity, group = day_id)) +
  geom_line(aes(color=day))
```

![](p8105_hw3_bsr2136_files/figure-gfm/Problem%203.3-1.png)<!-- -->

Some noticeable patterns/conclusions include:

  - During the middle of the day, there is the most activity on Saturday
    and Sunday

  - Towards the end of the day, there is the most activity on Monday and
    Friday

  - On many days there are clusters of higher than usual activity about
    400-480 minutes through the day, 600-750 minutes through the day,
    950-1050 minutes through the day, and 1200-1400 minutes through the
    day.

  - In general, the most activity seems to be on Saturday and Sunday
