---
title: "p8105_hw3_bsr2136"
author: "Barik Rajpal"
date: "10/10/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Problem 0

```{r Problem 0,warning=FALSE}
dir.create("files_for_hw3")
```

## Problem 1

### Describing the dataset

```{r Problem 1a}
library(p8105.datasets)
data("instacart")

## The number of rows in the dataset is
nrow(instacart)
## The number of columns in the dataset is
ncol(instacart)

## The dataset has identifying information such as order, product, and user.
## For example, The number of unique orders in the dataset is
length(unique(pull(instacart,order_id)))
## Which is equal to the number of unique users
length(unique(pull(instacart,user_id)))
## And the number of unique products is
length(unique(pull(instacart,product_id)))

## The dataset also gives information such as aisle and department of the
## product ordered. For example, The departments are
unique(pull(instacart,department))

## The dataset also gives the hour of day that the order was made.
## For example, the most common hour at which an order was made is
instacart %>% group_by(order_hour_of_day) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  .[1,1]
```

### Answering prompts about the dataset

#### 1. How many aisles are there, and which aisles are the most items ordered from?

```{r Problem 1.1}
## The number of aisles are
length(unique(pull(instacart,aisle)))
## And the aisles that the most items are ordered from are
instacart %>% group_by(aisle) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head()
```

There are 134 aisles total, and the 6 aisles that foods are most commonly ordered from are fresh vegetables, fresh fruits, packaged vegetables fruits, yogurt, packaged cheese, and water seltzer sparkling water. Among these 6, the top 2, fresh vegetables and fresh fruit, receive far more orders than the other 4 (almost 2 times the 3rd ranked aisle, packaged vegetables fruits)

#### 2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r Problem 1.2}
aisle_tbl <- instacart %>% 
  group_by(aisle) %>%
  summarise(n = n()) %>%
  filter(n>=10000) %>%
  arrange(n)

aisle_tbl %>%
  mutate(
    aisle = factor(pull(aisle_tbl,aisle),
                levels = pull(aisle_tbl,aisle)[order(pull(aisle_tbl,n))])) %>% 
  ggplot(aes(x=aisle,y=n)) + 
  geom_bar(stat = "identity") + 
  coord_flip() + xlab("Aisle") + ylab("Number of Items Ordered")
```

As also seen in the previous question, fresh vegetables and fresh fruits are ordered from far more than any other aisle. While there is a steep drop in number of items ordered after the highest 2 aisles, then a few more moderate drops over the next few aisles, the rate of decay slows greaty and from the eggs aisle to the butter aisle, (which looks to be ~25 aisles) there is a very small difference in the number of items ordered per aisle.

#### 3. Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r Problem 1.3}
baking <- instacart %>%
  filter(aisle == "baking ingredients") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

dog_food <- instacart %>%
  filter(aisle == "dog food care") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

pack_veg <- instacart %>%
  filter(aisle == "packaged vegetables fruits") %>%
  group_by(aisle,product_name) %>%
  summarise(number_of_times_ordered = n()) %>%
  arrange(desc(number_of_times_ordered)) %>%
  head(3)

rbind(baking,dog_food,pack_veg) %>%
  knitr::kable()
```

In addition to giving information about what are the 3 most popular items in each of the three aisles, this table gives very clear information on how popular the aisles are as well. The packaged vegetables fruits aisle is ordered from far more than the baking ingredients aisle, which is ordered from far more than the dog food care aisle.

#### 4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r Problem 1.4}
day_of_week <- instacart %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name,order_dow) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = order_dow,
              values_from = mean_hour)

names(day_of_week)[2:8] <- c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")

knitr::kable(day_of_week)
```

The mean time at which Coffee Ice Cream is ordered is between 1 and 2 on the weekend, between 3 and 4 from Tuesday through Thursday. Throughout the week, the mean time is between 12 and 4.

The mean time at which Pink Lady Apples are ordered is between 11 and 12 Monday, Tuesday, Thursday, and Saturday, and between 11 and 230 through the week.

## Problem 2

### Data Loading and Cleaning

```{r Problem 2a}
library(p8105.datasets)
data("brfss_smart2010")

unique(pull(brfss_smart2010,Locationabbr))
## This is just state, so I'm going to call it that

## I checked Locationdesc the same way, but I'm not showing it here to
## save space on the github_document.
## It is just county, so I'm going to call it that


names(brfss_smart2010)[2:3] = c("state","county")

brfss <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Poor","Fair","Good","Very good","Excellent")) %>%
  mutate(response = factor(response, levels = c("Poor","Fair","Good","Very good","Excellent")))

unique(pull(brfss,data_value_unit))
## So I'm going to get rid of that column and rename data_value to indicate that it is a %
unique(pull(brfss,location_id))
unique(pull(brfss,data_value_footnote_symbol))
## And get rid of those as well

names(brfss)[9] = "percent_responded"

brfss <- brfss %>%
  select(-topic,-data_value_unit,-location_id,-data_value_footnote_symbol)
```

### Answering questions about the dataset

#### 1. In 2002, which states were observed at 7 or more locations? What about in 2010?

```{r Problem 2.1}
brfss %>%
  filter(year==2002) %>%
  distinct(state,county) %>%
  group_by(state) %>%
  summarise(number_of_locations = n()) %>%
  filter(number_of_locations >=7)

brfss %>%
  filter(year==2010) %>%
  distinct(state,county) %>%
  group_by(state) %>%
  summarise(number_of_locations = n()) %>%
  filter(number_of_locations >=7)
```

In 2002, there were 6 states that were observed at 7 or more locations. PA was observed at 10 locations, NJ and MA were observed at 8 locations, and CT, FL, and NC were observed at 7 locations
In 2010, there were 14 states that were observed at 7 or more locations. FL, which was only observed in 7 locations in 2002, was observed at 41 locations in 2010, more than any other state. The others that were observed in 7 or more in 2002 were all also observed for 7 or more in 2010 except for CT.

#### 2. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r Problem 2.2}
brfss %>%
  filter(response == "Excellent") %>%
  filter(!is.na(percent_responded)) %>%
  group_by(year,state) %>%
  summarise(percent_excellent = mean(percent_responded)) %>%
  ggplot(aes(x=year,y=percent_excellent,color=state)) + 
  geom_line(aes(group = state))
```

The state (location) that consistently has close to the highest percent responding with excellent is DC (which is not techincally a state), and the state that consistently has close to the lowest percent responding with excellent is WV. While there is a lot of movement (or noise) over the years, states do not seem to dramatically switch ranks in terms of percent responding excellent over time.

#### 3. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r problem 2.3}
brfss_2006 <- brfss %>%
  filter(year==2006 & state == "NY") %>%
  ggplot(aes(x=response,y=percent_responded)) + 
  geom_boxplot() + ggtitle("2006") + 
  scale_y_continuous(limits = c(0,45))

brfss_2010 <- brfss %>%
  filter(year==2010 & state == "NY") %>%
  ggplot(aes(x=response,y=percent_responded)) + 
  geom_boxplot() + ggtitle("2010") + 
  scale_y_continuous(limits = c(0,45))

gridExtra::grid.arrange(brfss_2006,brfss_2010, nrow=1)
```

From 2006 to 2010, the amount of "Excellent" and "Very good" responses incrased on average, while the amount of "Fair" responses decreased on average.

## Problem 3

### Loading, tidying, and wrangling the data

```{r Problem 3.1}
accel <- read_csv("./files_for_hw3/accel_data.csv") %>%
  mutate(weekend = ifelse(day %in% c("Saturday","Sunday"),1,0),
         day = factor(day, levels = c("Sunday","Monday","Tuesday","Wednesday",
                                      "Thursday","Friday","Saturday"))) %>%
  select(weekend,everything()) %>%
  janitor::clean_names()
```

The resulting dataset has 35 observations of 1444 varibles, 1440 of those variables are the accelerator minute data. The other 4 are weekend, which is a boolean that indicates whether the day is a weekend, week, which is a numeric that indicates what week it is (1-5), day_id, which is a numeric that indicates what day it is (1-35), and day, which is a factor that indicates what day of the week it is (Sunday, Monday, etc.).

### Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r Problem 3.2}
accel_agg <- accel %>%
  mutate(total_activity = rowSums(accel[5:1444])) %>%
  select(day_id, total_activity) 

accel_agg %>%
  knitr::kable()
  
```

The first thing that stands out to me is that on day 24 and day 31, there total_activity values is 1440, which is the total number of minutes, which makes me suspect that the device was malfunctioning that day. Regarding trends, a high value is often followed by one or two low values, which maybe suggests that after 1 day of exercise, there was 1-2 days of rest.

### Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r Problem 3.3}

```

